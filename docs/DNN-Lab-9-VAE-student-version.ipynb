{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncOobWEl-FCH",
        "jukit_cell_id": "93DLcyzFQp"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc4H7ifG-FCI",
        "jukit_cell_id": "pIHQKkr9FK"
      },
      "source": [
        "## Variational AutoEncoders\n",
        "\n",
        "In this lab excercise you will train a Variational AutoEncoder to learn the distribution of the MNIST data. You will explore the latent space and learn how to generate new samples.\n",
        "\n",
        "Some notation:\n",
        "* $P^*$ is the true data distribution. We have some samples from this.\n",
        "* $p(z)$ is a *prior* distribution over the latent space. In our model it is multivariate gaussian distribution $N(0,\\mathbb{I})$.\n",
        "* $E(x)$ is the encoder that accepts data points as input and outputs distributions over the latent space $Z$. The produced distribution is denoted $q_\\phi(z|x)$ and is the (approximate) *posterior* distribution. In our model this is mutlivariate gaussian distribution $q_\\phi(z|x) \\sim N(\\mu, diag(\\sigma^2)$. Notes:\n",
        "    1. $\\phi$ are weights of the encoder network.\n",
        "    2. Encoder network accepts data points as input and outputs $\\mu$ and $\\sigma$, which are vectors of the same length as latent space. They are used to construct the approximate posterior distribution $q_\\phi(z|x)$.\n",
        "* $D(z)$ is the decoder that accepts samples from the latent distribution and output parameters of the the likelihood distribution $p_\\theta(x|z)$. In our model this is Bernoulli trial per each pixel $p_\\theta(x|z_0) \\sim Bern(p)$. Notes:\n",
        "    1. $\\theta$ are weights of the decoder network.\n",
        "    2. Decoder network accepts sample from the posterior distribution $q_\\phi(z|x)$ and outputs p, which is a matrix of the shape of the input image. Each value of the matrix is the parameter $\\pi$ of the Bernoulli trial $Bern(\\pi)$ for the corresponding pixel.\n",
        "    3. Data points are clipped to only contain values 0 and 1 so that the model could be trained in the given setup.\n",
        "\n",
        "Loss:\n",
        "The loss that is used is called ELBO (the Evidence Lower Bound).\n",
        "\n",
        "$$ELBO = \\mathbb{E}_{z \\sim q(z|x)} \\big[\\log p_\\theta(x|z)\\big] - \\mathbb{KL}\\big(q_\\phi(z | x) || p(z)\\big).$$\n",
        "\n",
        "The following equation holds:\n",
        "\n",
        "\n",
        "$$\\log p_{\\theta}(x) = ELBO + \\mathbb{KL}(q_\\theta(z|x) || p(z|x))$$\n",
        "\n",
        "Maximization of ELBO is equivalent of minimization of KL-divergence between to variational posterior distribution and the true posterior distribution.\n",
        "\n",
        "The first term of the loss is trained via stochastic gradient descent. The second term can be calculated analytically in our setup and is equal to:\n",
        "\n",
        "$$ \\mathbb{KL}\\big( \\mathcal{N}(\\mu, \\sigma^2) || \\mathcal{N}(0, 1) \\big) = \\frac12 \\big(\\sigma^2  - \\log(\\sigma^2) + \\mu^2 - 1 \\big).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXUastP5-FCJ",
        "jukit_cell_id": "RtyXwx9fPx"
      },
      "source": [
        "Tasks for the tutorial:\n",
        "1. Run the pipeline and verify that VAE is training and generating decent digit representation.\n",
        "2. Play with training parameters and / or network layers to better learn hidden representation of the data\n",
        "3. Implement sample_latent method in the VariationalAutoEncoder class, which accepts original image as input and outputs samples from the posterior distribution $q_\\phi(z|x)$.\n",
        "4. Implement sample method in the VariationalAutoEncoder class, which accepts sample size and optionally samples from the prior distribution. as input and outputs samples:\n",
        "    1. If samples are not avialable, take a sample $z_0 \\sim p(z)$ from the prior distribution.\n",
        "    2. Decode the latent $p_\\theta(x|z_0) = D_\\theta(z_0)$.\n",
        "    3. Sample a reconstruction from the likelihood: $x_0 \\sim p_\\theta(x|z_0)$.\n",
        "5. Explore the latent space. For each class encode a sample (>=100) of images of that class and take one parameters from the posterior distribution $q_\\phi(z|x)$ per image. Visualize samples as scatter plot. Remember to color points according to image classes!\n",
        "5. Sample two points $z_0, z_1$ from the prior distibution $p(z)$. Perform interpolation i.e. visualize how samples change based on points from segment ended by $z_0$ and $z_1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wY7V_y6e-FCJ",
        "jukit_cell_id": "pDhDmcelEe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms # type: ignore\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XefdMpOj-FCJ",
        "jukit_cell_id": "EiwAAcwFio"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "test_batch_size = 1000\n",
        "epochs = 5\n",
        "lr = 5e-3\n",
        "seed = 1\n",
        "log_interval = 5\n",
        "latent_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "R0sfJjEh-FCK",
        "jukit_cell_id": "Z87FGYM0il"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {'batch_size': batch_size}\n",
        "test_kwargs = {'batch_size': test_batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 1,\n",
        "                    'pin_memory': True,\n",
        "                    'shuffle': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NZ5G3DxB-FCK",
        "jukit_cell_id": "xrVvSGrkSL"
      },
      "outputs": [],
      "source": [
        "def visualize_data(\n",
        "    images: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    max_images: int,\n",
        "    max_fig_size=(30, 30)\n",
        "):\n",
        "\n",
        "    num_frames, num_channels, h, w, = images.shape\n",
        "    num_frames = min(num_frames, max_images)\n",
        "    ff, axes = plt.subplots(1, num_frames,\n",
        "                            figsize=max_fig_size,\n",
        "                            subplot_kw={'xticks': [], 'yticks': []})\n",
        "    if num_frames == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i in range(0, num_frames):\n",
        "        if num_channels == 3:\n",
        "            axes[i].imshow(np.squeeze(images[i]))\n",
        "        else:\n",
        "            axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n",
        "        if labels is not None:\n",
        "            axes[i].set_title(labels[i].item(), fontsize=28)\n",
        "        plt.setp(axes[i].get_xticklabels(), visible=False)\n",
        "        plt.setp(axes[i].get_yticklabels(), visible=False)\n",
        "    ff.subplots_adjust(wspace=0.1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YymQPDD5-FCK",
        "jukit_cell_id": "ZR40BQxeGP"
      },
      "outputs": [],
      "source": [
        "class Binarize:\n",
        "    def __call__(self, sample: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.bernoulli(sample)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    Binarize()\n",
        "])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Eb_G_gyc-FCK",
        "jukit_cell_id": "D3EMM5HuMy",
        "outputId": "bfcc2e91-1c82-437f-81d0-25e8d7ac1b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3000x3000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "real_batch = next(iter(train_loader))\n",
        "visualize_data(real_batch[0], real_batch[1], 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "V5uJu6vj-FCK",
        "jukit_cell_id": "lDWaCw7ncK"
      },
      "outputs": [],
      "source": [
        "EncoderOutput = namedtuple(\"EncoderOutput\", [\"mu\", \"sigma\"])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        linear_sizes: list[int],\n",
        "        latent_size: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for in_layer_size, out_layer_size in zip(linear_sizes, linear_sizes[1:]):\n",
        "            self.layers.append(nn.Linear(in_layer_size, out_layer_size))\n",
        "            self.layers.append(nn.BatchNorm1d(out_layer_size))\n",
        "            self.layers.append(nn.ReLU())\n",
        "\n",
        "        self.last_layer_mu = nn.Linear(linear_sizes[-1], latent_size)\n",
        "        self.last_layer_sigma = nn.Linear(linear_sizes[-1], latent_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = nn.Flatten()(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        mu = self.last_layer_mu(x)\n",
        "        logsigma = self.last_layer_sigma(x)\n",
        "        return EncoderOutput(mu, torch.log(1 + torch.exp(logsigma)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lfsCDhcQ-FCK",
        "jukit_cell_id": "gOtr8l8qF4"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        linear_sizes: list[int],\n",
        "        output_size: tuple[int]\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for in_layer_size, out_layer_size in zip(linear_sizes, linear_sizes[1:]):\n",
        "            self.layers.append(nn.Linear(in_layer_size, out_layer_size))\n",
        "            self.layers.append(nn.BatchNorm1d(out_layer_size))\n",
        "            self.layers.append(nn.ReLU())\n",
        "\n",
        "        self.last_layer = nn.Sequential(\n",
        "            nn.Linear(linear_sizes[-1], output_size[0] * output_size[1]),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        for layer in self.layers:\n",
        "            z = layer(z)\n",
        "\n",
        "        x = self.last_layer(z)\n",
        "\n",
        "        x = x.view(-1, 1, *self.output_size)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aaESKiTG-FCL",
        "jukit_cell_id": "bhd31Y9iWU"
      },
      "outputs": [],
      "source": [
        "VariationalAutoEncoderOutput = namedtuple(\"VariationalAutoEncoderOutput\", [\"mu\", \"sigma\", \"p\"])\n",
        "\n",
        "\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_linear_sizes: list[int],\n",
        "        latent_size: int,\n",
        "        decoder_linear_sizes: list[int],\n",
        "        output_size: tuple[int]\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(encoder_linear_sizes, latent_size)\n",
        "        self.decoder = Decoder(decoder_linear_sizes, output_size)\n",
        "        self.latent_size = latent_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        encoded = self.encoder(x)\n",
        "\n",
        "        z = torch.normal(0., 1., size=list(encoded.mu.size())).to(device)\n",
        "        z = (z * encoded.sigma) + encoded.mu\n",
        "\n",
        "        decoded = self.decoder(z)\n",
        "        return VariationalAutoEncoderOutput(encoded.mu, encoded.sigma, decoded)\n",
        "\n",
        "    def sample_latent(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        encoded = self.encoder(x)\n",
        "        mu, sigma = encoded.mu, encoded.sigma\n",
        "\n",
        "        epsilon = torch.randn_like(sigma).to(x.device)\n",
        "\n",
        "        z = mu + sigma * epsilon\n",
        "\n",
        "        return z\n",
        "\n",
        "    def sample(self, sample_size: int, samples=None) -> torch.Tensor:\n",
        "        if samples is None:\n",
        "            samples = torch.randn(sample_size, self.latent_size).to(next(self.parameters()).device)\n",
        "\n",
        "        decoded = self.decoder(samples)\n",
        "\n",
        "        epsilon = torch.randn_like(decoded).to(decoded.device)\n",
        "        x0 = decoded + epsilon\n",
        "\n",
        "        return x0\n",
        "\n",
        "\n",
        "    def interpolate(self, z0: torch.Tensor, z1: torch.Tensor, steps: int = 10) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform interpolation between two latent points z0 and z1.\n",
        "\n",
        "        Args:\n",
        "            z0 (torch.Tensor): Starting point in the latent space.\n",
        "            z1 (torch.Tensor): Ending point in the latent space.\n",
        "            steps (int): Number of interpolation steps.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Decoded samples along the interpolation path.\n",
        "        \"\"\"\n",
        "        alphas = torch.linspace(0, 1, steps).to(z0.device)\n",
        "\n",
        "        interpolated_z = torch.outer(alphas, z1 - z0) + z0.unsqueeze(0)\n",
        "\n",
        "        decoded_samples = self.decoder(interpolated_z)\n",
        "\n",
        "        return decoded_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4_r1lpXa-FCL",
        "jukit_cell_id": "lvLfQuqbgf"
      },
      "outputs": [],
      "source": [
        "def KL_gaussian_loss(mu, sigma):\n",
        "    return torch.mean(((sigma * sigma) - (2 * torch.log(sigma)) + (mu * mu) - 1) / 2)\n",
        "\n",
        "def ELBO(x, p, mu, sigma):\n",
        "    BCE = F.binary_cross_entropy(p, x)\n",
        "    KL = KL_gaussian_loss(mu, sigma)\n",
        "    return BCE + KL * 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Zk9JPfZX-FCL",
        "jukit_cell_id": "yDxpIMfrnA"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, device: torch.device, train_loader: DataLoader,\n",
        "          optimizer: optim.Optimizer, epoch: int, log_interval: int):\n",
        "    model.train()\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = ELBO(data, output.p, output.mu, output.sigma)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model: nn.Module, device: torch.device, test_loader: DataLoader):\n",
        "    model.eval()\n",
        "    test_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, _) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            loss = ELBO(data, output.p, output.mu, output.sigma)\n",
        "            test_loss = test_loss + (loss * data.size(0))\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "m-WSVyBv-FCL",
        "jukit_cell_id": "8z8DaWFxZw"
      },
      "outputs": [],
      "source": [
        "vae = VariationalAutoEncoder([28 * 28, 500, 350], latent_size, [latent_size, 350, 500], (28, 28))\n",
        "vae.to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "and-7B8q-FCM",
        "jukit_cell_id": "0EgbJq8zsj",
        "outputId": "0f09c599-c760-4fc1-ed44-de1e6885ff84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.734024\n",
            "Train Epoch: 1 [5120/60000 (8%)]\tLoss: 0.295833\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.280959\n",
            "Train Epoch: 1 [15360/60000 (25%)]\tLoss: 0.264460\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.259792\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    train(vae, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(vae, device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1644Gbj-FCM",
        "jukit_cell_id": "GZhv2SnWvs"
      },
      "outputs": [],
      "source": [
        "vae.eval()\n",
        "visualize_data(\n",
        "    vae(real_batch[0].to(device)).p.detach().cpu().numpy(),\n",
        "    labels=real_batch[1].cpu().numpy(),\n",
        "    max_images=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14ZTUm0P-FCM",
        "jukit_cell_id": "zR2QvB46U2"
      },
      "outputs": [],
      "source": [
        "visualize_data(\n",
        "    torch.bernoulli(vae(real_batch[0].to(device)).p).detach().cpu().numpy(),\n",
        "    labels=real_batch[1].cpu().numpy(),\n",
        "    max_images=8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSZ7dwSN-FCM",
        "jukit_cell_id": "3u0aaKb096"
      },
      "source": [
        "Visualization of latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_MRrHCj-FCM",
        "jukit_cell_id": "ac2sV6BoJf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def explore_latent_space(model, dataloader, num_samples=100, device='cpu'):\n",
        "    model.eval()\n",
        "    latent_vectors = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, class_labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            class_labels = class_labels.to(device)\n",
        "\n",
        "            encoded = model.encoder(images)\n",
        "            mu = encoded.mu\n",
        "\n",
        "            latent_vectors.append(mu.cpu().numpy())\n",
        "            labels.append(class_labels.cpu().numpy())\n",
        "\n",
        "    latent_vectors = np.vstack(latent_vectors)\n",
        "    labels = np.concatenate(labels)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(latent_vectors[:, 0], latent_vectors[:, 1], c=labels, cmap='tab10', alpha=0.6)\n",
        "    plt.colorbar(scatter, label='Class Labels')\n",
        "    plt.title('Latent Space Visualization')\n",
        "    plt.xlabel('Latent Dimension 1')\n",
        "    plt.ylabel('Latent Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "explore_latent_space(vae, train_loader, num_samples=100, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta0YPwc5-FCM",
        "jukit_cell_id": "GpGpgLNLS6"
      },
      "source": [
        "Sample interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzC_7MUS-FCM",
        "jukit_cell_id": "oy0iwiaUEv"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "z0 = torch.randn(1, vae.latent_size).to('cuda')\n",
        "z1 = torch.randn(1, vae.latent_size).to('cuda')\n",
        "\n",
        "# Perform interpolation\n",
        "steps = 10\n",
        "interpolated_samples = vae.interpolate(z0, z1, steps=steps)\n",
        "\n",
        "# Plot interpolated images\n",
        "plt.figure(figsize=(15, 3))\n",
        "for i, sample in enumerate(interpolated_samples):\n",
        "    plt.subplot(1, steps, i + 1)\n",
        "    plt.imshow(sample.squeeze().cpu().detach().numpy(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMyu71gY-FCM",
        "jukit_cell_id": "EwZ5GE7s8b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}